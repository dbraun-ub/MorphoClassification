import csv
import os
import pickle

import click
import numpy as np
import torch

from preprocess_utils import plot_graph
from torch_geometric.loader import DataLoader

import utils
from shapes_gcn import SingleViewGATv2, MyDataset


def str_to_list(ctx, param, value):
    """
    It's a callback function used with Click options
    to split the string by comma and convert each part to float
    """
    try:
        if value:
            return [v for v in value.split(",")]
    except ValueError:
        raise click.BadParameter("Names must be separated by commas.")


@click.command()
@click.option(
    "--train_root",
    required=True,
    help="Path to a numpy array containing GPA aligned cofiguration matrices - All views.",
)
@click.option(
    "--edges_view",
    default="edges_SD",
    help="Dictionary key that contains the graph model (another dictionary) as a value.",
)
@click.option(
    "--graph_model",
    default="Rep1",
    help="Graph representation model - may contain multi-view nodes connected in different ways.",
)
@click.option(
    "--lr", default=2e-4, help="Number of epochs to train.",
)
@click.option(
    "--num_epochs", default=21, help="Number of epochs to train.",
)
@click.option(
    "--resume_from", default=0, help="The epoch of the checkpoint to be resumed.",
)
@click.option(
    "--save_checkpoint",
    default=20,
    help="Save checkpoints every defined number of epochs.",
)
@click.option(
    "--configpath",
    default="config/dev-config.yml",
    help="Path to the configuration file.",
)
@click.option(
    "--num_feature_labels",
    default="age,height,weight,bmi",
    callback=str_to_list,
    help="A string with the name of the features separated by commas.",
)
@click.option(
    "--target_labels",
    default="SD_DIST1,SD_DIST2,SD_DIST3,SD_DIST4,SD_DIST5",
    callback=str_to_list,
    help="A string with the name of the targets separated by commas.",
)
@click.option(
    "--extra_nodes",
    default=None,
    callback=str_to_list,
    help="A string with the name of the extra nodes separated by commas.",
)
@click.option(
    "--remove_nodes",
    default=None,
    callback=str_to_list,
    help="A string with the name of nodes to remove separated by commas.",
)
def run_train(
    train_root: str,
    edges_view: str,
    graph_model: str,
    lr: float,
    num_epochs: int,
    resume_from: int,
    save_checkpoint: int,
    configpath: str,
    num_feature_labels: list,
    target_labels: list,
    extra_nodes: list,
    remove_nodes: list,
):

    config = utils.load_config(configpath)

    """
    Create InMemoryDataset
    """
    # Three files generated by train_preproc.py
    train_files = ["confs_train.npy", "demog_train.npy", "sd_params_train.npy"]

    train_dataset = MyDataset(
        root=train_root + "/train",
        raw_files=train_files,
        edge_view=edges_view,
        edge_model=graph_model,
        config=config,
        num_feature_labels=num_feature_labels,
        target_labels=target_labels,
        extra_nodes=extra_nodes,
        remove_nodes=remove_nodes,
    )
    """
    Preview graph representation to be trained
    """
    plot_graph(train_dataset[0])

    """
    Keep track of the normalization parameters (mean and standard deviation)
    so we can denormalize the predictions later on
    Save the dictionary
    """

    # Check if the params.pickle exists
    if not os.path.exists(train_root + "/" + "params.pickle"):
        # Create the file
        with open(train_root + "/" + "params.pickle", "wb") as f:
            pickle.dump(train_dataset.params, f)

        print("params.pickle is created")
    else:
        print("params.pickle already exists")

    num_global_features = len(num_feature_labels) + 2  # sex = 2 variables
    num_classes = len(target_labels)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = SingleViewGATv2(
        num_node_features=2,  # x,y
        num_global_features=num_global_features,  # sex,age,height,weight,bmi
        hidden_channels=64,
        num_classes=num_classes,
    ).to(device)
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = torch.nn.MSELoss()

    print(model)

    # Load checkpoint if it exists
    try:
        checkpoint = torch.load(
            train_root + "/" + "checkpoint_{}.pth".format(resume_from), device
        )
        print("Checkpoint loaded")
        model.load_state_dict(checkpoint["model_state_dict"])
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])

    except FileNotFoundError:
        print("No checkpoint found")

    # Load trainlog if it exists
    if os.path.exists(train_root + "/" + "trainlog.csv"):
        rows = []
        # Open and read the CSV file
        with open(train_root + "/" + "trainlog.csv", mode="r", newline="") as file:
            csv_reader = csv.reader(file)
            for row in csv_reader:
                # Join the row elements into a single string separated by commas
                row_string = ",".join(row)
                rows.append(row_string)
        print("Resuming trainlog...")

    else:
        rows = []

        rows.append(f"# train={train_dataset.y.shape[0]}")
        rows.append(f"# lr={lr}")
        rows.append(f"# edge_view={edges_view}")
        rows.append(f"# edge_model={graph_model}")
        rows.append(f"# extra_nodes={extra_nodes}")
        rows.append(f"# remove_nodes={remove_nodes}")
        rows.append(f"# target_labels={target_labels}")
        rows.append(f"# num_feature_labels={num_feature_labels}")
        rows.append("epoch,loss")

        print("epoch,loss")
        print(f"# lr={lr}")

    for epoch in range(num_epochs):
        total_loss = 0

        # for data in train_dataset:  # Iterate over batches of the dataset

        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

        for data in train_loader:
            data = data.to(device)  # Move each batch of data to the device
            optimizer.zero_grad()  # Clear gradients
            out = model(data)  # Perform a forward pass through the model
            loss = loss_fn(out, data.y)  # Compute the loss
            loss.backward()  # Backpropagation
            optimizer.step()  # Update the weights

            total_loss += loss.item()  # Add the loss for this batch

        # Print loss for this epoch
        # print(f"Epoch: {epoch+1+resume_from}, Loss: {total_loss / len(train_dataset)}")
        epoch_to_str = epoch + 1 + resume_from
        loss_to_str = np.array(total_loss / len(train_dataset)).round(5)
        print("{},{}".format(epoch_to_str, loss_to_str))
        rows.append("{},{}".format(epoch_to_str, loss_to_str))

        if (
            epoch % save_checkpoint == 0
        ):  # Save a checkpoint every "save_checkpoint" epochs
            torch.save(
                {
                    "epoch": epoch + resume_from,
                    "model_state_dict": model.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                    "loss": loss,
                },
                f"{train_root + '/'}checkpoint_{epoch+resume_from}.pth",
            )

        file_to_write = "\n".join(rows) + "\n"
        with open(train_root + "/" + "trainlog.csv", "w") as f:
            f.write(file_to_write)


if __name__ == "__main__":
    run_train()
